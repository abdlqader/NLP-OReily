{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chunker.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tniM5T-lzYVt",
        "colab_type": "text"
      },
      "source": [
        "for a simple information extraction system. It begins by processing a document using several of the procedures discussed in Chapters 3 and 5: first, the raw text of the document is split into sentences using a sentence segmenter, and each sentence is further subdivided into words using a tokenizer. Next, each sentence is tagged with part-of-speech tags, which will prove very helpful in the next step, named entity recognition. In this step, we search for mentions of potentially interesting entities in each sentence. Finally, we use relation recognition to search for likely relations between different entities in the text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzoPYUhcKMVy",
        "colab_type": "code",
        "outputId": "c9a080fb-b528-4444-f9b2-2d925b0ee8dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#lets make function of a preprocessing\n",
        "import nltk\n",
        "import re\n",
        "import pprint\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifu_Frri0996",
        "colab_type": "text"
      },
      "source": [
        "lets first talk about chunking a group of tokens that refere to multipe nouns and dont neccessarily refer to entities the same way as definite NPs and proper names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNbMvOjMKYtb",
        "colab_type": "code",
        "outputId": "171a39cd-468d-4f89-b3ee-d76b8cd2c455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#chunks in IOB formate\n",
        "from nltk.corpus import conll2000\n",
        "#corpus of chunked sentence in IOB formate\n",
        "nltk.download('conll2000')\n",
        "cp = nltk.RegexpParser(\"\")\n",
        "#NP since we are only interested in nouns\n",
        "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
        "print(cp.evaluate(test_sents))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "ChunkParse score:\n",
            "    IOB Accuracy:  43.4%%\n",
            "    Precision:      0.0%%\n",
            "    Recall:         0.0%%\n",
            "    F-Measure:      0.0%%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EupOVkYvAY2z",
        "colab_type": "text"
      },
      "source": [
        "The IOB tag accuracy indicates that more than a third of the words are tagged with O, i.e., not in an NP chunk. However, since our tagger did not find any chunks, its precision, recall, and F-measure are all zero. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV0wpF24KaDN",
        "colab_type": "code",
        "outputId": "4aa9ee5c-a581-4ecb-b2f0-2f52f7f94737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#lets try a naive rergular expression that looks for tags beginning with letters that are characteristic of noun phrase tags\n",
        "#CD, DT and JJ\n",
        "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "print(cp.evaluate(test_sents))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  87.7%%\n",
            "    Precision:     70.6%%\n",
            "    Recall:        67.8%%\n",
            "    F-Measure:     69.2%%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYF_8PvABaEG",
        "colab_type": "text"
      },
      "source": [
        "decent results however we can improve by adopting a more data-driven approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjgdABq6Bgvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnigramChunker(nltk.ChunkParserI):\n",
        "  def __init__(self,train_sents):\n",
        "    train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n",
        "    #we could make it bigramchunker which increases performanc slightly more than unigram\n",
        "    self.tagger = nltk.UnigramTagger(train_data)\n",
        "    print(train_data[0])\n",
        "  def parse(self,sentence):\n",
        "    pos_tags = [pos for (word,pos) in sentence]\n",
        "    tagged_pos_tags =  self.tagger.tag(pos_tags)\n",
        "    chunktags = [chunktag for (pos,chunktag) in tagged_pos_tags]\n",
        "    conlltags = [(word,pos,chunktag) for ((word,pos),chunktag) in zip(sentence,chunktags)]\n",
        "    return nltk.chunk.conlltags2tree(conlltags)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d10SJwmwGdJU",
        "colab_type": "code",
        "outputId": "20893f39-bfd8-4b91-80b2-c0e54052c11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "test_sents = conll2000.chunked_sents('test.txt',chunk_types=['NP'])\n",
        "train_sents = conll2000.chunked_sents('train.txt',chunk_types=['NP'])\n",
        "unigram_chunker = UnigramChunker(train_sents)\n",
        "print(unigram_chunker.evaluate(test_sents))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('NN', 'B-NP'), ('IN', 'O'), ('DT', 'B-NP'), ('NN', 'I-NP'), ('VBZ', 'O'), ('RB', 'O'), ('VBN', 'O'), ('TO', 'O'), ('VB', 'O'), ('DT', 'B-NP'), ('JJ', 'I-NP'), ('NN', 'I-NP'), ('IN', 'O'), ('NN', 'B-NP'), ('NNS', 'I-NP'), ('IN', 'O'), ('NNP', 'B-NP'), (',', 'O'), ('JJ', 'O'), ('IN', 'O'), ('NN', 'B-NP'), ('NN', 'B-NP'), (',', 'O'), ('VB', 'O'), ('TO', 'O'), ('VB', 'O'), ('DT', 'B-NP'), ('JJ', 'I-NP'), ('NN', 'I-NP'), ('IN', 'O'), ('NNP', 'B-NP'), ('CC', 'I-NP'), ('NNP', 'I-NP'), ('POS', 'B-NP'), ('JJ', 'I-NP'), ('NNS', 'I-NP'), ('.', 'O')]\n",
            "ChunkParse score:\n",
            "    IOB Accuracy:  92.9%%\n",
            "    Precision:     79.9%%\n",
            "    Recall:        86.8%%\n",
            "    F-Measure:     83.2%%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VKRu72KSytA",
        "colab_type": "text"
      },
      "source": [
        "if u wanna read more about how to auto chunk, the book made an example with 96% IOB accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhNROyFDTiBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "310293ff-3ff2-4dc8-d183-622fea320b3b"
      },
      "source": [
        "#back to named entity recognition\n",
        "#there is a pretrained classifier provided by nltk trained to recognize names entities accessed with the\n",
        "#function nltk.ne_chunk()\n",
        "#If we set the parameter binary=True , then named entities are just tagged as NE; \n",
        "#otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE.\n",
        "nltk.download('maxent_ne_chunker') \n",
        "nltk.download('treebank')\n",
        "nltk.download('words')\n",
        "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
        "print(nltk.ne_chunk(sent,binary=True))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "(S\n",
            "  The/DT\n",
            "  (NE U.S./NNP)\n",
            "  is/VBZ\n",
            "  one/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  few/JJ\n",
            "  industrialized/VBN\n",
            "  nations/NNS\n",
            "  that/WDT\n",
            "  *T*-7/-NONE-\n",
            "  does/VBZ\n",
            "  n't/RB\n",
            "  have/VB\n",
            "  a/DT\n",
            "  higher/JJR\n",
            "  standard/NN\n",
            "  of/IN\n",
            "  regulation/NN\n",
            "  for/IN\n",
            "  the/DT\n",
            "  smooth/JJ\n",
            "  ,/,\n",
            "  needle-like/JJ\n",
            "  fibers/NNS\n",
            "  such/JJ\n",
            "  as/IN\n",
            "  crocidolite/NN\n",
            "  that/WDT\n",
            "  *T*-1/-NONE-\n",
            "  are/VBP\n",
            "  classified/VBN\n",
            "  *-5/-NONE-\n",
            "  as/IN\n",
            "  amphobiles/NNS\n",
            "  ,/,\n",
            "  according/VBG\n",
            "  to/TO\n",
            "  (NE Brooke/NNP)\n",
            "  T./NNP\n",
            "  Mossman/NNP\n",
            "  ,/,\n",
            "  a/DT\n",
            "  professor/NN\n",
            "  of/IN\n",
            "  pathlogy/NN\n",
            "  at/IN\n",
            "  the/DT\n",
            "  (NE University/NNP)\n",
            "  of/IN\n",
            "  (NE Vermont/NNP College/NNP)\n",
            "  of/IN\n",
            "  (NE Medicine/NNP)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}